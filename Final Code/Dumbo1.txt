To run PCA :
 
def profileUS1(line : String):String = {
val d = "[\t]+"
val str = line.split(d)
var r = ""
for(i <- 13 to (str.length-1)) {
r = r+"\t"+str(i)
}
r
}

val USdata1 = sc.textFile("/user/db3619/spark/project/data1.txt")

val Pdata1 = USdata1.map(line => profileUS1(line))


def bully_PCA(line : String): String = {
val d = "[\t]+"
val str = line.split(d)
var s = str(173)
for(i <- 174 to 184){
s = s +" "+ str(i)
}
s
}

val bully_stat = Pdata1.map(line => bully_PCA(line))

def getDouble(x : String):Double = {
if((x.toInt != -9) && (x.toInt != 1) && (x.toInt != 2) ) {
1.0
}
else
{
0.0
}
}


import org.apache.spark.mllib.feature.PCA
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.rdd.RDD

val data = bully_stat.map{ line =>
val d = "[ ]+"
val parts = line.split(d)
LabeledPoint(getDouble(parts(0)), Vectors.dense(parts.map(x => getDouble(x))))
}


val pca = new PCA(1).fit(data.map(_.features))
val projected = data.map(p => p.copy(features = pca.transform(p.features)))

val mean_cal = projected.map(_.features.toString)
val m = mean_cal.map{ s =>
val d = "[ \\[\\]]+"
s.split(d)(1).toDouble
}

val mean_v = m.sum/m.count()
val diff = m.map(v => (v - mean_v)*(v - mean_v))

val stddev = Math.sqrt(diff.sum / m.count())

val left = mean_v + stddev
val right = mean_v - stddev


val final_output = m.map{ d =>
if( (d<=left) && (d >= right))
0.0
else
1.0
}

final_output.saveAsTextFile("/user/db3619/spark/project/output")



To run normal : 

def bully(line : String):String = {
      val d = "[\t]+"
      val str = line.split(d)
      var r = "0"
      for(i <- 186 to 197) {
        if(str(i).toInt > 1)
        {
          r = "1"
        }
      }
    line + "\t" + r
    }

val USdata1 = sc.textFile("/user/db3619/spark/project/data1.txt")

val Pdata1 = USdata1.map(line => bully(line))
